x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile-Airflow
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy
  env_file:
    # - ./env/clickhouse.env
    # - ./env/minio.env
    # - ./env/minio.creds
    - ./env/spark.env
  environment: &airflow-common-env
    AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
    AIRFLOW_UID: ${AIRFLOW_UID}
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: 'Asia/Tehran'
    _PIP_ADDITIONAL_REQUIREMENTS: ''
    DASHBOARD_API_URL: http://dashboard-api:8080/report
  image: lp/airflow
  networks:
    - lp
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
  volumes:
    - ./airflow/config:/opt/airflow/config
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/dbt:/opt/airflow/electronics_data_test
    - airflow-data:/opt/airflow

x-healthcheck:
  healthcheck: &healthcheck-common
    interval: 30s
    timeout: 5s
    retries: 3
    start_period: 30s

services:   

  ################################
  # Nifi Service
  ################################ 
  nifi:
    image: apache/nifi:1.27.0    
    ports:
      - "8085:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_WEB_HTTP_HOST=0.0.0.0
      - NIFI_JVM_HEAP_INIT=512m
      - NIFI_JVM_HEAP_MAX=1g
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - lp

  ################################
  # airflow Services
  ################################ 

  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_started
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      # test: ["CMD", "curl", "--fail", "http://localhost:8080/monitor/health"]  # https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html
    ports:
      - "8080:8080"
    restart: always
  
  airflow-init:
    <<: *airflow-common    
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          export AIRFLOW_UID=$$(id -u)
        fi

        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"

        if (( mem_available < 4000 )) ; then warning_resources="true"; fi
        if (( cpus_available < 2 )); then warning_resources="true"; fi
        if (( disk_available < one_meg * 10 )); then warning_resources="true"; fi

        mkdir -p /opt/airflow/{logs,dags,plugins,config}
        mkdir /opt/airflow/logs/spark

        echo "Airflow version:"
        /entrypoint airflow version

        ls -la /opt/airflow/{logs,dags,plugins,config}

        /entrypoint airflow config list >/dev/null

        chown -R "${AIRFLOW_UID}:0" /opt/airflow/

        echo "Waiting for Airflow DB..."
        until airflow db check; do
          >&2 echo "Airflow DB not reachable. Waiting..."
          sleep 5
        done

        echo "Add/Update Spark connection"
        airflow connections add "$SPARK_CONN_NAME" \
          --conn-type "spark" \
          --conn-host "spark://spark-master" \
          --conn-port "7077"

        echo "Done."
        exit 0;

    depends_on:
      <<: *airflow-common-depends-on
    entrypoint: /bin/bash
    
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"    

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_started
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
    restart: always
  
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_started
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
    restart: always
  
  airflow-worker:
    <<: *airflow-common
    command: celery worker
    depends_on:
      <<: *airflow-common-depends-on
      airflow-apiserver:
        condition: service_healthy
      airflow-init:
        condition: service_started
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    healthcheck:
      <<: *healthcheck-common
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
    restart: always
    hostname: airflow-worker

  ################################
  # Kafka Services
  ################################ 

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8089:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    networks:
      - lp

  connect:
    image: confluentinc/cp-kafka-connect:7.9.1
    hostname: connect
    container_name: connect
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: broker:29092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_ID: connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      
      # Key & Value Converters
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081        
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      
      # Plugin path
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components      
      
      # Performance & Reliability && Dead Letter Queue Configs
      CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS: 30000
      CONNECT_ERRORS_TOLERANCE: all
      CONNECT_ERRORS_LOG_ENABLE: "true"
      CONNECT_ERRORS_LOG_INCLUDE_MESSAGES: "true"

      CONNECT_ERRORS_DEADLETTERQUEUE_TOPIC_NAME: dlq-sales-events
      CONNECT_ERRORS_DEADLETTERQUEUE_TOPIC_REPLICATION_FACTOR: 1
      CONNECT_ERRORS_DEADLETTERQUEUE_CONTEXT_HEADERS_ENABLE: "true"

      # AWS credentials (set these in env/all.env or your environment)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
    volumes:
      - ./connectors:/usr/share/confluent-hub-components
      - ./env/aws.creds:/home/appuser/.aws/credentials/aws.creds
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - lp
    restart: unless-stopped

  topic-creator:
    image: confluentinc/cp-kafka:7.9.1-1-ubi8
    depends_on:
      broker:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Creating/ensuring topic sales_events_avro with 12 partitions...'
        kafka-topics --bootstrap-server broker:29092 --create --if-not-exists \
          --topic sales_events_avro --partitions 12 --replication-factor 1 \
          --config cleanup.policy=delete --config retention.ms=604800000 || true
        echo 'Topic ready!'
        sleep infinity
      "
    networks:
      - lp

  producer:
    build:
      context: ./producer
      dockerfile: Dockerfile-Producer
    command: ["python", "producer.py"]
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy  
    env_file:
      - ./env/kafka.env
      - ./env/postgres.env    
      - ./env/postgres.creds
    image: lp/producer
    networks:
      - lp
    restart: unless-stopped
    environment:
      # Kafka    
      KAFKA_BOOTSTRAP_SERVERS: broker:29092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      KAFKA_TOPIC: sales_events_avro

      # Postgres
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}      
      NUM_WORKERS: 2

    volumes:
      - ./producer:/home/app        
    working_dir: /home/app  

  broker:
    env_file:
      - ./env/kafka.env
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENERS: 'PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: 'var/lib/kafka/data'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    image: confluentinc/cp-kafka:7.9.1-1-ubi8
    healthcheck:
      <<: *healthcheck-common
      test: ['CMD', 'kafka-broker-api-versions', '--bootstrap-server', 'broker:29092']
    hostname: broker
    networks:
      - lp
    restart: always
    volumes:
      - broker:/var/lib/kafka/data
  
  broker-helper:
    command:
      - -c
      - |
        echo "Waiting for broker..."
        until kafka-topics --bootstrap-server broker:29092 --list; do sleep 2; done
        echo "broker available. Checking $${KAFKA_TOPIC} topic..."
        if kafka-topics --bootstrap-server broker:29092 --list | grep -q "^$${KAFKA_TOPIC}$$"; then
          echo "Topic exists. Checking partitions count..."
          TOPIC_INFO=$$(kafka-topics --bootstrap-server broker:29092 --describe --topic $${KAFKA_TOPIC})
          PARTITION_COUNT=$$(echo \"$$TOPIC_INFO\" | grep -oE "PartitionCount: [0-9]+" | awk "{print $$2}")
          echo \"Current partition count: $$PARTITION_COUNT\"
          if [ \"$$PARTITION_COUNT\" != \"6\" ]; then
            echo "Changing partition count to 6..."
            kafka-topics --bootstrap-server broker:29092 --alter --topic $${KAFKA_TOPIC} --partitions 6
            echo "Partition count updated to 6"
          else
            echo "Partition count is already 6"
          fi
        else
          echo "Creating $${KAFKA_TOPIC} topic with 6 partitions..."
          kafka-topics --bootstrap-server broker:29092 --create --topic $${KAFKA_TOPIC} --partitions 6 --replication-factor 1
        fi
        echo "Topic info:"
        kafka-topics --bootstrap-server broker:29092 --describe --topic $${KAFKA_TOPIC}
        echo "Done"
        exit 0;
    entrypoint: /bin/bash
    env_file:
      - ./env/kafka.env
    depends_on:
      broker:
        condition: service_healthy
    image: confluentinc/cp-kafka:7.9.1-1-ubi8
    networks:
      - lp      
  
  consumer:  # This is just for debugging
    command:
      - -c
      - |
        echo "Starting Kafka Consumer..."
        kafka-console-consumer --bootstrap-server broker:29092 --topic $${KAFKA_TOPIC} --from-beginning
    depends_on:
      - broker
    entrypoint: /bin/bash
    env_file:
      - ./env/kafka.env
    environment:
      KAFKA_BOOTSTRAP_SERVERS: 'PLAINTEXT://broker:29092'
      KAFKA_GROUP_ID: my-debug-consumer-group
      KAFKA_AUTO_OFFSET_RESET: earliest
      KAFKA_ENABLE_AUTO_COMMIT: 'true'
    image: confluentinc/cp-kafka:7.9.1-1-ubi8
    networks:
      - lp
    profiles:
      - debug            
  
  schema-registry:
    depends_on:
      broker:
        condition: service_started
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://broker:29092'
    hostname: schema-registry
    image: confluentinc/cp-schema-registry:7.9.1-1-ubi8.amd64
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
    networks:
      - lp
    restart: always

  ################################
  # Databases Services
  ################################ 

  postgres:
    env_file:
      - ./env/postgres.env
      - ./env/postgres.creds
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
    image: postgres:17.4-bookworm
    networks:
      - lp
    restart: always
    volumes:
      - postgres:/var/lib/postgresql/data
    
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data
    healthcheck:              
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5  
    networks:
      - lp  
  cassandra:
    image: cassandra:4.0
    container_name: cassandra
    hostname: cassandra
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=SalesCluster
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - lp
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -u cassandra -p cassandra -e 'describe keyspaces'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: always  
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_LISTEN_PORT: 80
    ports:
      - "5433:80"     
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      - postgres
    networks:
      - lp  

################################
# Spark Services
################################ 

  spark-notebook:
    image: quay.io/jupyter/all-spark-notebook:spark-3.5.3
    container_name: spark-notebook
    restart: always
    ports:
      - "8888:8888"
      - "4040:4040"
    volumes:
      - ./:/home/jovyan/work
    environment:
      - JUPYTER_TOKEN=easy
    user: root
    networks:
      - lp

  # ==========================================
  # STORAGE & VISUALIZATION LAYERS
  # ==========================================      
    
  sink-service:
    build:
      context: ./cassandra
      dockerfile: Dockerfile-Sink
    container_name: sink-service
    depends_on:
      cassandra:
        condition: service_healthy
      broker:
        condition: service_healthy
    networks:
      - lp
    restart: always  


  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    hostname: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=hadesarchitect-cassandra-datasource
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - lp
    depends_on:
      cassandra:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always    

networks:
  lp:
    name: lp

volumes:
  broker:  
  postgres:
  redis_data:
  pgadmin_data:
  nifi_conf:
  spark_data:
  cassandra_data:
  grafana_data:
  airflow-data: